{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Selection\n",
    "\n",
    "When exploring the optimal neural network architecture for the KMNIST dataset, our hyperparameter tuning proved not to be as effective as the prediction that comes from the architecture developed. Hence, we decide to choose the popularly used ADAM optimizer for our training during exploration. We decide to set the L2 regularisation weight decays to 0.0 and to choose a learning rate of 1e-4 (a commonly used value [cite]). \n",
    "\n",
    "Having decided on an optimal neural network architecture we progress to decide running an optimal weight decay and learning rate search for our ADAM optimizer. Given the limited time on the project, performing a grid search of every combination of the weight decay ranging from () and learning rate ranging from () would take up to .... searches. Hence we use a mixed search strategy to find the optimal parameters in relatively less time with considerably better hyperparameters to before. \n",
    "\n",
    "The strategy involves running a random search parameter of up to 15 iterations and then explore the domain around the best random search parameters using grid search. With this we go from a naive grid search of __ to __ searches. \n",
    "\n",
    "The found hyperparameters were __ = __ and __ = __ . Using this on our model architecture gave us a validation prediction of __!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Approach\n",
    "\n",
    "To train our model, we use the ADAM optimizer for our network gradient minimisations. This is because the ADAM optimizer has shown to be one of the better performing optimizers in practice and literature [cite]. At this stage of the pipeline, in the network architecture process, we keep to using only commonly used hyperparameters. We use an L2 Lambda Regularisation of 0.0 and a learning rate of 1e-4. Due to time limitations we use 60 epochs for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
